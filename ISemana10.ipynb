{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUWA5SebOk9Hh7ZUtPnsxZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ledyquesada/Ejercicios/blob/main/ISemana10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NupthW0x3mIm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TÉCNICAS AVANZADAS DE OPTIMIZACIÓN EN MACHINE LEARNING\n",
        "**SGD**"
      ],
      "metadata": {
        "id": "wqdXZVig3peS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCION SGD\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def stochastic_gradient_descent(data, labels, model, learning_rate=0.01, epochs=100):\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(len(data)):\n",
        "            random_index = random.randint(0, len(data) - 1)\n",
        "            x, y = data[random_index], labels[random_index]\n",
        "            gradient = compute_gradient(x, y, model)\n",
        "            model = update_parameters(model, gradient, learning_rate)\n",
        "    return model\n",
        "\n",
        "\n",
        "#COMPARACION\n",
        "\n",
        "# Descenso de Gradiente Batch\n",
        "def batch_gradient_descent(data, labels, model, learning_rate=0.01, epochs=100):\n",
        "    for epoch in range(epochs):\n",
        "        gradient = compute_gradient(data, labels, model)\n",
        "        model = update_parameters(model, gradient, learning_rate)\n",
        "    return model\n",
        "\n",
        "# Descenso de Gradiente Mini-Batch\n",
        "def mini_batch_gradient_descent(data, labels, model, batch_size=32, learning_rate=0.01, epochs=100):\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            mini_batch = data[i:i+batch_size]\n",
        "            mini_labels = labels[i:i+batch_size]\n",
        "            gradient = compute_gradient(mini_batch, mini_labels, model)\n",
        "            model = update_parameters(model, gradient, learning_rate)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "8Vn6y2PC31zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OPTIMIZADORES POPULARES**"
      ],
      "metadata": {
        "id": "NIaYb1lR47pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Momentum\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "sgd_momentum = SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(optimizer=sgd_momentum, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#AdaGrad\n",
        "\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "\n",
        "adagrad = Adagrad(learning_rate=0.01)\n",
        "model.compile(optimizer=adagrad, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#RMSProp\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "rmsprop = RMSprop(learning_rate=0.001, rho=0.9)\n",
        "model.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Adam\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DDpXuMky5BZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ajuste de hiperparámetros**"
      ],
      "metadata": {
        "id": "Jqw9UYnC-D07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tasa de aprendizaje\n",
        "\n",
        "#Manual\n",
        "\n",
        "learning_rates = [0.1, 0.01, 0.001]\n",
        "for lr in learning_rates:\n",
        "    optimizer = SGD(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "#Busqueda en cuadricula\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "def create_model(learning_rate=0.01):\n",
        "    optimizer = SGD(learning_rate=learning_rate)\n",
        "    model = Sequential()\n",
        "    # Definir la arquitectura del modelo\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
        "learning_rates = [0.1, 0.01, 0.001]\n",
        "param_grid = dict(learning_rate=learning_rates)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "#Momento\n",
        "\n",
        "#Manual\n",
        "\n",
        "momenta = [0.5, 0.9, 0.99]\n",
        "for momentum in momenta:\n",
        "    optimizer = SGD(learning_rate=0.01, momentum=momentum)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "\n",
        "#Aleatoria\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def create_model(momentum=0.9):\n",
        "    optimizer = SGD(learning_rate=0.01, momentum=momentum)\n",
        "    model = Sequential()\n",
        "    # Definir la arquitectura del modelo\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
        "momenta = [0.5, 0.9, 0.99]\n",
        "param_dist = dict(momentum=momenta)\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, scoring='accuracy', cv=3)\n",
        "random_result = random_search.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "LD28vvWv-Ine"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfer Learning**"
      ],
      "metadata": {
        "id": "EOxHffaSBrbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#extraccion de caracterisitcas\n",
        "\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Congelar capas del modelo base\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Utilizar el modelo base como extractor de características\n",
        "model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)\n",
        "\n",
        "#Ajuste fino\n",
        "\n",
        "# Descongelar las últimas capas para ajuste fino\n",
        "for layer in base_model.layers[-4:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compilar el modelo para ajuste fino\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Ajuste fino en el nuevo conjunto de datos\n",
        "model.fit(new_data, epochs=10)\n",
        "\n",
        "# Metodos avanzados\n",
        "\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Definir la entrada para las dos imágenes\n",
        "input_a = Input(shape=(224, 224, 3))\n",
        "input_b = Input(shape=(224, 224, 3))\n",
        "\n",
        "# Construir la red siamesa\n",
        "base_model = VGG16(weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "output = Dense(256)(x)\n",
        "\n",
        "siamese_model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Utilizar la misma instancia de modelo siamesa para ambas imágenes\n",
        "output_a = siamese_model(input_a)\n",
        "output_b = siamese_model(input_b)\n"
      ],
      "metadata": {
        "id": "n6AezqftBuv_"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}